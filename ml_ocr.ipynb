{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abderrazzaq-laanoui/PFE-HexaCoders/blob/main/ml_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8hCO76eiaTU"
      },
      "source": [
        "<div style=\"text-align:center\"><h1>RECONNAISSANCE DE L'ÉCRITURE MANUSCRITE DE LESTTRES TIFINAGH</h1></div>\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CACEY3QXpYdB"
      },
      "source": [
        "# dépendances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiufKkBMpkLy"
      },
      "source": [
        "#### Liaison avec Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrxPNHSdpikm",
        "outputId": "77b99f2c-d305-4572-c45b-5cc593847b28"
      },
      "source": [
        "# liaison avec  Google Drive comme source de donnees \r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puJQ-cszpuEL"
      },
      "source": [
        "#### importations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDPSleVSptAh"
      },
      "source": [
        "# Importation des bibliothèques\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import cv2 as cv\r\n",
        "import random\r\n",
        "import pickle\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import time\r\n",
        "from datetime import timedelta\r\n",
        "import math\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_8SKXLvwgFA"
      },
      "source": [
        "#### Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYOaYFjlwlDU"
      },
      "source": [
        "\r\n",
        "CATEGORIES = [\"ya\", \"yab\", \"yach\", \"yad\", \"yadd\", \"yae\", \"yaf\", \"yag\", \"yagh\", \r\n",
        "              \"yagw\", \"yah\", \"yahh\", \"yaj\", \"yak\", \"yakw\", \"yal\", \"yam\", \"yan\", \r\n",
        "              \"yaq\", \"yar\",\"yarr\", \"yas\", \"yass\", \"yat\", \"yatt\", \"yaw\", \"yax\", \r\n",
        "              \"yay\", \"yaz\", \"yazz\", \"yey\", \"yi\", \"yu\"] # a list of all possible classes\r\n",
        "T_CATEGORIES = ['ⴰ', 'ⴱ', 'ⵛ', 'ⴷ', 'ⴹ', 'ⵄ', 'ⴼ', 'ⴳ', 'ⵖ', 'ⴳⵯ', 'ⵀ', 'ⵃ', 'ⵊ', 'ⴽ', 'ⴽⵯ',\r\n",
        "                'ⵍ','ⵎ','ⵏ', 'ⵇ', 'ⵔ', 'ⵕ', 'ⵙ', 'ⵚ', 'ⵜ', 'ⵟ', 'ⵡ', 'ⵅ', 'ⵢ', 'ⵣ','ⵥ', 'ⴻ', 'ⵉ', 'ⵓ']\r\n",
        "\r\n",
        "training_data = [] # cette liste qui contiendra les données d'entraînement traitées\r\n",
        "testing_data = [] # cette liste qui contiendra les données de test traitées"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp2YMmNBHo0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e4bead-edd2-459d-eba8-4591ea60d23f"
      },
      "source": [
        "for i in range(33):\r\n",
        "    print(CATEGORIES[i] + ' => ' + T_CATEGORIES[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ya => ⴰ\n",
            "yab => ⴱ\n",
            "yach => ⵛ\n",
            "yad => ⴷ\n",
            "yadd => ⴹ\n",
            "yae => ⵄ\n",
            "yaf => ⴼ\n",
            "yag => ⴳ\n",
            "yagh => ⵖ\n",
            "yagw => ⴳⵯ\n",
            "yah => ⵀ\n",
            "yahh => ⵃ\n",
            "yaj => ⵊ\n",
            "yak => ⴽ\n",
            "yakw => ⴽⵯ\n",
            "yal => ⵍ\n",
            "yam => ⵎ\n",
            "yan => ⵏ\n",
            "yaq => ⵇ\n",
            "yar => ⵔ\n",
            "yarr => ⵕ\n",
            "yas => ⵙ\n",
            "yass => ⵚ\n",
            "yat => ⵜ\n",
            "yatt => ⵟ\n",
            "yaw => ⵡ\n",
            "yax => ⵅ\n",
            "yay => ⵢ\n",
            "yaz => ⵣ\n",
            "yazz => ⵥ\n",
            "yey => ⴻ\n",
            "yi => ⵉ\n",
            "yu => ⵓ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT29UQrnd5G0"
      },
      "source": [
        "\r\n",
        "# CHARGEMENT DU DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl4_nyb6goN6"
      },
      "source": [
        "\r\n",
        "#@markdown \\\r\n",
        "#@markdown #### Entrez le chemin d'accès au répertoire DATASET dans votre Drive:\r\n",
        "chemin = \"\" #@param {type:\"string\"}\r\n",
        "#@markdown \\\r\n",
        "\r\n",
        "if chemin == \"\":\r\n",
        "    chemin = \"genie info/Rapport PFE - HexaCoders/DATASET\" #default path\r\n",
        "# Declaration des Variables\r\n",
        "DIR = os.path.join(\"/content/drive/MyDrive\",chemin) # h the path to the DATASET dir  in your drive\r\n",
        "TRAIN_DATA_DIR = os.path.join(DIR,\"training_data/\")\r\n",
        "TEST_DATA_DIR = os.path.join(DIR,\"testing_data/\")\r\n",
        "MODEL_DIR = os.path.join(DIR,\"model/\")\r\n",
        "TRAIN_MODEL_DIR = os.path.join(DIR,\"model/train/\")\r\n",
        "TEST_MODEL_DIR = os.path.join(DIR,\"model/test/\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdc2JFHU77S7"
      },
      "source": [
        "\r\n",
        "## DONNÉES D'ENTRAÎNEMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N_yFjf_qXCP"
      },
      "source": [
        "\r\n",
        "#### Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S-Vd6uGsYcO"
      },
      "source": [
        "<b>OpenCV</b> (référencé par cv) est une bibliothèque multi-plateforme qui nous permet de développer des applications de vision par ordinateur en temps réel. Elle se concentre principalement sur le traitement des images en temps réel., la capture et l'analyse vidéo, y compris des fonctionnalités telles que la détection des visages et des objets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTDF8xVxkhrs"
      },
      "source": [
        "training_data = [] # la liste qui contiendra les données d'entraînement traitées\r\n",
        "def prepare_training_data():\r\n",
        "    for category in CATEGORIES :\r\n",
        "        path = os.path.join(TRAIN_DATA_DIR, category)\r\n",
        "        class_num = CATEGORIES.index(category)  # obtention du classement (0 à 32)\r\n",
        "        for img in os.listdir(path): # itérer sur chaque image dans un dossier de caractères\r\n",
        "            img_array = cv.imread(os.path.join(path, img), cv.IMREAD_GRAYSCALE) # read graysacled images\r\n",
        "            new_array = cv.resize(img_array, (50,50)) # redimensionnement des images\r\n",
        "            training_data.append([new_array, class_num]) # append processed image to the training data list\r\n",
        "\r\n",
        "prepare_training_data() # calling the fct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZslzhLOz64Ib"
      },
      "source": [
        "testing_data = [] # a list that will contain processed testing data\r\n",
        "def prepare_testing_data():\r\n",
        "    for category in CATEGORIES :\r\n",
        "        path = os.path.join(TEST_DATA_DIR, category)\r\n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 to 32)\r\n",
        "        for img in os.listdir(path): # iterate over each image in a caracter folder \r\n",
        "            img_array = cv.imread(os.path.join(path, img), cv.IMREAD_GRAYSCALE) # read graysacled images\r\n",
        "            new_array = cv.resize(img_array, (50,50)) # resizing images\r\n",
        "            testing_data.append([new_array, class_num]) # append processed image to the testing data list\r\n",
        "\r\n",
        "prepare_testing_data() # calling the fct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbCC07Y3coSH"
      },
      "source": [
        "Nous voulons mélanger les données. Pour l'instant, nos données sont sous forme de : \"ya,\"yab\", puis \"yach\" ... . Cela finit généralement par causer des problèmes, car, au début, le classificateur apprendra à prédire toujours \"ya\". Ensuite, il passera à prédire \"yab\" ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4nf3AKE7GDh"
      },
      "source": [
        "# shuffle the  data randomly \r\n",
        "# random.shuffle(training_data)\r\n",
        "random.shuffle(testing_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-_F3RpVwN5d"
      },
      "source": [
        "\r\n",
        "#### Création du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1vgeaDU7fHa"
      },
      "source": [
        "# Créer le modèle\r\n",
        "x_train = [] \r\n",
        "y_train = []\r\n",
        "\r\n",
        "for features,label in training_data:\r\n",
        "    x_train.append(features)\r\n",
        "    y_train.append(label)\r\n",
        "\r\n",
        "x_train = np.array(x_train)\r\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfg7eK6g88Xg"
      },
      "source": [
        "# Créer le modèle\r\n",
        "x_test = [] \r\n",
        "y_test = []\r\n",
        "\r\n",
        "for features,label in testing_data:\r\n",
        "    x_test.append(features)\r\n",
        "    y_test.append(label)\r\n",
        "\r\n",
        "x_test = np.array(x_test)\r\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeYWj6QXqgeu"
      },
      "source": [
        "\r\n",
        "#### Sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4lC8VIOb_kY"
      },
      "source": [
        "Le module <b> pickle </b> met en œuvre des protocoles binaires pour sérialiser et désérialiser une structure d'objet Python. <br><br> Le \"pickling\" est le processus par lequel une hiérarchie d'objets Python est convertie en un flux d'octets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzzQfQdu7rnu"
      },
      "source": [
        "\r\n",
        "# Sauvegarder le modèle\r\n",
        "\r\n",
        "pickle_out = open(TRAIN_MODEL_DIR + \"x_train.pickle\",\"wb\")\r\n",
        "pickle.dump(x_train, pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_out = open(TRAIN_MODEL_DIR + \"y_train.pickle\",\"wb\")\r\n",
        "pickle.dump(y_train, pickle_out)\r\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_g1sU_9Okt"
      },
      "source": [
        "# Sauvegarder le modèle\r\n",
        "\r\n",
        "pickle_out = open(TEST_MODEL_DIR + \"x_test.pickle\",\"wb\")\r\n",
        "pickle.dump(x_test, pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_out = open(TEST_MODEL_DIR + \"y_test.pickle\",\"wb\")\r\n",
        "pickle.dump(y_test, pickle_out)\r\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9s-a1jwqqQQ"
      },
      "source": [
        "\r\n",
        "#### Téléchargement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PRFC3SurZOH"
      },
      "source": [
        "Le \"unpickling\" est l'opération inverse, par laquelle un flux d'octets (provenant d'un fichier binaire ou d'un objet de type octet) est reconverti en une hiérarchie d'objets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8tLrB3_72kp"
      },
      "source": [
        "# to load the model we use the script\r\n",
        "pickle_in = open(TRAIN_MODEL_DIR + \"x_train.pickle\",\"rb\")\r\n",
        "x_train = pickle.load(pickle_in)\r\n",
        "\r\n",
        "pickle_in = open(TRAIN_MODEL_DIR + \"y_train.pickle\",\"rb\")\r\n",
        "y_train = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwSHBy139ayT"
      },
      "source": [
        "# to load the model we use the script\r\n",
        "pickle_in = open(TEST_MODEL_DIR + \"x_test.pickle\",\"rb\")\r\n",
        "x_test = pickle.load(pickle_in)\r\n",
        "\r\n",
        "pickle_in = open(TEST_MODEL_DIR + \"y_test.pickle\",\"rb\")\r\n",
        "y_test = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04UMVpJcIDiv"
      },
      "source": [
        "# CRÉER LE RÉSEAU NURAL CONVOLUTIONNEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaQ6iTWVBtzS"
      },
      "source": [
        "#### Helper-function for plotting images\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBcIZgZKBssY"
      },
      "source": [
        "def plot_image(image, cls_true, cls_pred=None):\r\n",
        "\r\n",
        "    plt.imshow(image.reshape(img_shape), cmap='gray')\r\n",
        "    \r\n",
        "    # Show true and predicted classes.\r\n",
        "    if cls_pred is None:\r\n",
        "        plt.figtext(.5,0,\"True: {0}\".format(T_CATEGORIES[cls_true]), fontsize=14, ha='center')\r\n",
        "    else:\r\n",
        "          plt.figtext(.5,0, \"True: {0}, Pred: {1}\".format(T_CATEGORIES[cls_true], T_CATEGORIES[cls_pred]), fontsize=14, ha='center')\r\n",
        "        \r\n",
        "    plt.axis('off')\r\n",
        "\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM8TPLkXr7Mu"
      },
      "source": [
        "#### CNN Configurations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUp1KS7HxZRD"
      },
      "source": [
        "# Convolutional Layer 1.\r\n",
        "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\r\n",
        "num_filters1 = 16         # There are 16 of these filters.\r\n",
        "\r\n",
        "# Convolutional Layer 2.\r\n",
        "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\r\n",
        "num_filters2 = 36         # There are 36 of these filters.\r\n",
        "\r\n",
        "# Fully-connected layer.\r\n",
        "fc_size = 128             # Number of neurons in fully-connected layer."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I5wu7iWBicv"
      },
      "source": [
        "#### Dimensions de Donnees pour plus de commodité"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2A6pz5O_l0k"
      },
      "source": [
        "# The number of pixels in each dimension of an image.\r\n",
        "img_size = 50\r\n",
        "\r\n",
        "# The images are stored in one-dimensional arrays of this length.\r\n",
        "img_size_flat = 50*50\r\n",
        "\r\n",
        "# Tuple with height and width of images used to reshape arrays.\r\n",
        "img_shape = (50, 50)\r\n",
        "\r\n",
        "# Number of classes, one class for each of 33 alphabet.\r\n",
        "num_classes = 33\r\n",
        "\r\n",
        "# Number of colour channels for the images: 1 channel for gray-scale.\r\n",
        "num_channels = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKc7mXkVQHP5"
      },
      "source": [
        "\r\n",
        "#### Structuration des entrées"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k1w_AkRNO5g"
      },
      "source": [
        "# to reduce the computition we convert the numbers from float64 => float32\r\n",
        "# we normalise (deviding all by 255.0) to make numbers between 0 => 1 instead of 0 => 255\r\n",
        "x_train = x_train.reshape(-1,img_size_flat).astype(\"float32\") / 255.0 \r\n",
        "x_test = x_test.reshape(-1,img_size_flat).astype(\"float32\") / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}