{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abderrazzaq-laanoui/PFE-HexaCoders/blob/main/ml_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8hCO76eiaTU"
      },
      "source": [
        "<div style=\"text-align:center\"><h1>RECONNAISSANCE DE L'ÉCRITURE MANUSCRITE DE LESTTRES TIFINAGH</h1></div>\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CACEY3QXpYdB"
      },
      "source": [
        "# dépendances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiufKkBMpkLy"
      },
      "source": [
        "#### Liaison avec Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrxPNHSdpikm",
        "outputId": "e36fb8e4-2881-4fcd-ee89-1dcfac13df81"
      },
      "source": [
        "# liaison avec  Google Drive comme source de donnees \r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puJQ-cszpuEL"
      },
      "source": [
        "#### importations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDPSleVSptAh"
      },
      "source": [
        "# Importation des bibliothèques\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import cv2 as cv\r\n",
        "import random\r\n",
        "import pickle\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import time\r\n",
        "from datetime import timedelta\r\n",
        "import math\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_8SKXLvwgFA"
      },
      "source": [
        "#### Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYOaYFjlwlDU"
      },
      "source": [
        "\r\n",
        "CATEGORIES = [\"ya\", \"yab\", \"yach\", \"yad\", \"yadd\", \"yae\", \"yaf\", \"yag\", \"yagh\", \r\n",
        "              \"yagw\", \"yah\", \"yahh\", \"yaj\", \"yak\", \"yakw\", \"yal\", \"yam\", \"yan\", \r\n",
        "              \"yaq\", \"yar\",\"yarr\", \"yas\", \"yass\", \"yat\", \"yatt\", \"yaw\", \"yax\", \r\n",
        "              \"yay\", \"yaz\", \"yazz\", \"yey\", \"yi\", \"yu\"] # a list of all possible classes\r\n",
        "T_CATEGORIES = ['ⴰ', 'ⴱ', 'ⵛ', 'ⴷ', 'ⴹ', 'ⵄ', 'ⴼ', 'ⴳ', 'ⵖ', 'ⴳⵯ', 'ⵀ', 'ⵃ', 'ⵊ', 'ⴽ', 'ⴽⵯ',\r\n",
        "                'ⵍ','ⵎ','ⵏ', 'ⵇ', 'ⵔ', 'ⵕ', 'ⵙ', 'ⵚ', 'ⵜ', 'ⵟ', 'ⵡ', 'ⵅ', 'ⵢ', 'ⵣ','ⵥ', 'ⴻ', 'ⵉ', 'ⵓ']\r\n",
        "\r\n",
        "training_data = [] # cette liste qui contiendra les données d'entraînement traitées\r\n",
        "testing_data = [] # cette liste qui contiendra les données de test traitées"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp2YMmNBHo0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e4bead-edd2-459d-eba8-4591ea60d23f"
      },
      "source": [
        "for i in range(33):\r\n",
        "    print(CATEGORIES[i] + ' => ' + T_CATEGORIES[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ya => ⴰ\n",
            "yab => ⴱ\n",
            "yach => ⵛ\n",
            "yad => ⴷ\n",
            "yadd => ⴹ\n",
            "yae => ⵄ\n",
            "yaf => ⴼ\n",
            "yag => ⴳ\n",
            "yagh => ⵖ\n",
            "yagw => ⴳⵯ\n",
            "yah => ⵀ\n",
            "yahh => ⵃ\n",
            "yaj => ⵊ\n",
            "yak => ⴽ\n",
            "yakw => ⴽⵯ\n",
            "yal => ⵍ\n",
            "yam => ⵎ\n",
            "yan => ⵏ\n",
            "yaq => ⵇ\n",
            "yar => ⵔ\n",
            "yarr => ⵕ\n",
            "yas => ⵙ\n",
            "yass => ⵚ\n",
            "yat => ⵜ\n",
            "yatt => ⵟ\n",
            "yaw => ⵡ\n",
            "yax => ⵅ\n",
            "yay => ⵢ\n",
            "yaz => ⵣ\n",
            "yazz => ⵥ\n",
            "yey => ⴻ\n",
            "yi => ⵉ\n",
            "yu => ⵓ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT29UQrnd5G0"
      },
      "source": [
        "\r\n",
        "# CHARGEMENT DU DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl4_nyb6goN6"
      },
      "source": [
        "\r\n",
        "#@markdown \\\r\n",
        "#@markdown #### Entrez le chemin d'accès au répertoire DATASET dans votre Drive:\r\n",
        "chemin = \"\" #@param {type:\"string\"}\r\n",
        "#@markdown \\\r\n",
        "\r\n",
        "if chemin == \"\":\r\n",
        "    chemin = \"genie info/Rapport PFE - HexaCoders/DATASET\" #default path\r\n",
        "# Declaration des Variables\r\n",
        "DIR = os.path.join(\"/content/drive/MyDrive\",chemin) # h the path to the DATASET dir  in your drive\r\n",
        "TRAIN_DATA_DIR = os.path.join(DIR,\"training_data/\")\r\n",
        "TEST_DATA_DIR = os.path.join(DIR,\"testing_data/\")\r\n",
        "MODEL_DIR = os.path.join(DIR,\"model/\")\r\n",
        "TRAIN_MODEL_DIR = os.path.join(DIR,\"model/train/\")\r\n",
        "TEST_MODEL_DIR = os.path.join(DIR,\"model/test/\")\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdc2JFHU77S7"
      },
      "source": [
        "\r\n",
        "## DONNÉES D'ENTRAÎNEMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N_yFjf_qXCP"
      },
      "source": [
        "\r\n",
        "#### Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S-Vd6uGsYcO"
      },
      "source": [
        "<b>OpenCV</b> (référencé par cv) est une bibliothèque multi-plateforme qui nous permet de développer des applications de vision par ordinateur en temps réel. Elle se concentre principalement sur le traitement des images en temps réel., la capture et l'analyse vidéo, y compris des fonctionnalités telles que la détection des visages et des objets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTDF8xVxkhrs"
      },
      "source": [
        "training_data = [] # la liste qui contiendra les données d'entraînement traitées\r\n",
        "def prepare_training_data():\r\n",
        "    for category in CATEGORIES :\r\n",
        "        path = os.path.join(TRAIN_DATA_DIR, category)\r\n",
        "        class_num = CATEGORIES.index(category)  # obtention du classement (0 à 32)\r\n",
        "        for img in os.listdir(path): # itérer sur chaque image dans un dossier de caractères\r\n",
        "            img_array = cv.imread(os.path.join(path, img), cv.IMREAD_GRAYSCALE) # read graysacled images\r\n",
        "            new_array = cv.resize(img_array, (50,50)) # redimensionnement des images\r\n",
        "            training_data.append([new_array, class_num]) # append processed image to the training data list\r\n",
        "\r\n",
        "prepare_training_data() # calling the fct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZslzhLOz64Ib"
      },
      "source": [
        "testing_data = [] # a list that will contain processed testing data\r\n",
        "def prepare_testing_data():\r\n",
        "    for category in CATEGORIES :\r\n",
        "        path = os.path.join(TEST_DATA_DIR, category)\r\n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 to 32)\r\n",
        "        for img in os.listdir(path): # iterate over each image in a caracter folder \r\n",
        "            img_array = cv.imread(os.path.join(path, img), cv.IMREAD_GRAYSCALE) # read graysacled images\r\n",
        "            new_array = cv.resize(img_array, (50,50)) # resizing images\r\n",
        "            testing_data.append([new_array, class_num]) # append processed image to the testing data list\r\n",
        "\r\n",
        "prepare_testing_data() # calling the fct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbCC07Y3coSH"
      },
      "source": [
        "Nous voulons mélanger les données. Pour l'instant, nos données sont sous forme de : \"ya,\"yab\", puis \"yach\" ... . Cela finit généralement par causer des problèmes, car, au début, le classificateur apprendra à prédire toujours \"ya\". Ensuite, il passera à prédire \"yab\" ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4nf3AKE7GDh"
      },
      "source": [
        "# shuffle the  data randomly \r\n",
        "# random.shuffle(training_data)\r\n",
        "random.shuffle(testing_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-_F3RpVwN5d"
      },
      "source": [
        "\r\n",
        "#### Création du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1vgeaDU7fHa"
      },
      "source": [
        "# Créer le modèle\r\n",
        "x_train = [] \r\n",
        "y_train = []\r\n",
        "\r\n",
        "for features,label in training_data:\r\n",
        "    x_train.append(features)\r\n",
        "    y_train.append(label)\r\n",
        "\r\n",
        "x_train = np.array(x_train)\r\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfg7eK6g88Xg"
      },
      "source": [
        "# Créer le modèle\r\n",
        "x_test = [] \r\n",
        "y_test = []\r\n",
        "\r\n",
        "for features,label in testing_data:\r\n",
        "    x_test.append(features)\r\n",
        "    y_test.append(label)\r\n",
        "\r\n",
        "x_test = np.array(x_test)\r\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeYWj6QXqgeu"
      },
      "source": [
        "\r\n",
        "#### Sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4lC8VIOb_kY"
      },
      "source": [
        "Le module <b> pickle </b> met en œuvre des protocoles binaires pour sérialiser et désérialiser une structure d'objet Python. <br><br> Le \"pickling\" est le processus par lequel une hiérarchie d'objets Python est convertie en un flux d'octets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzzQfQdu7rnu"
      },
      "source": [
        "\r\n",
        "# Sauvegarder le modèle\r\n",
        "\r\n",
        "pickle_out = open(TRAIN_MODEL_DIR + \"x_train.pickle\",\"wb\")\r\n",
        "pickle.dump(x_train, pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_out = open(TRAIN_MODEL_DIR + \"y_train.pickle\",\"wb\")\r\n",
        "pickle.dump(y_train, pickle_out)\r\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_g1sU_9Okt"
      },
      "source": [
        "# Sauvegarder le modèle\r\n",
        "\r\n",
        "pickle_out = open(TEST_MODEL_DIR + \"x_test.pickle\",\"wb\")\r\n",
        "pickle.dump(x_test, pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_out = open(TEST_MODEL_DIR + \"y_test.pickle\",\"wb\")\r\n",
        "pickle.dump(y_test, pickle_out)\r\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9s-a1jwqqQQ"
      },
      "source": [
        "\r\n",
        "#### Téléchargement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PRFC3SurZOH"
      },
      "source": [
        "Le \"unpickling\" est l'opération inverse, par laquelle un flux d'octets (provenant d'un fichier binaire ou d'un objet de type octet) est reconverti en une hiérarchie d'objets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8tLrB3_72kp"
      },
      "source": [
        "# to load the model we use the script\r\n",
        "pickle_in = open(TRAIN_MODEL_DIR + \"x_train.pickle\",\"rb\")\r\n",
        "x_train = pickle.load(pickle_in)\r\n",
        "\r\n",
        "pickle_in = open(TRAIN_MODEL_DIR + \"y_train.pickle\",\"rb\")\r\n",
        "y_train = pickle.load(pickle_in)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwSHBy139ayT"
      },
      "source": [
        "# to load the model we use the script\r\n",
        "pickle_in = open(TEST_MODEL_DIR + \"x_test.pickle\",\"rb\")\r\n",
        "x_test = pickle.load(pickle_in)\r\n",
        "\r\n",
        "pickle_in = open(TEST_MODEL_DIR + \"y_test.pickle\",\"rb\")\r\n",
        "y_test = pickle.load(pickle_in)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04UMVpJcIDiv"
      },
      "source": [
        "# CRÉER LE RÉSEAU NURAL CONVOLUTIONNEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaQ6iTWVBtzS"
      },
      "source": [
        "#### Helper-function for plotting images\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBcIZgZKBssY"
      },
      "source": [
        "def plot_image(image, cls_true, cls_pred=None):\r\n",
        "\r\n",
        "    plt.imshow(image.reshape(img_shape), cmap='gray')\r\n",
        "    \r\n",
        "    # Show true and predicted classes.\r\n",
        "    if cls_pred is None:\r\n",
        "        plt.figtext(.5,0,\"True: {0}\".format(T_CATEGORIES[cls_true]), fontsize=14, ha='center')\r\n",
        "    else:\r\n",
        "          plt.figtext(.5,0, \"True: {0}, Pred: {1}\".format(T_CATEGORIES[cls_true], T_CATEGORIES[cls_pred]), fontsize=14, ha='center')\r\n",
        "        \r\n",
        "    plt.axis('off')\r\n",
        "\r\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM8TPLkXr7Mu"
      },
      "source": [
        "#### CNN Configurations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I5wu7iWBicv"
      },
      "source": [
        "#### Dimensions de Donnees pour plus de commodité"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2A6pz5O_l0k"
      },
      "source": [
        "# The number of pixels in each dimension of an image.\r\n",
        "img_size = 50\r\n",
        "\r\n",
        "# The images are stored in one-dimensional arrays of this length.\r\n",
        "img_size_flat = 50*50\r\n",
        "\r\n",
        "# Tuple with height and width of images used to reshape arrays.\r\n",
        "img_shape = (50, 50)\r\n",
        "\r\n",
        "# Number of classes, one class for each of 33 alphabet.\r\n",
        "num_classes = 33\r\n",
        "\r\n",
        "# Number of colour channels for the images: 1 channel for gray-scale.\r\n",
        "num_channels = 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKc7mXkVQHP5"
      },
      "source": [
        "\r\n",
        "#### Structuration des entrées"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k1w_AkRNO5g"
      },
      "source": [
        "# to reduce the computition we convert the numbers from float64 => float32\r\n",
        "# we normalise (deviding all by 255.0) to make numbers between 0 => 1 instead of 0 => 255\r\n",
        "x_train = x_train.reshape(-1,50,50,1).astype(\"float32\") / 255.0 \r\n",
        "x_test = x_test.reshape(-1,50,50,1).astype(\"float32\") / 255.0"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDznGxmqAwJB"
      },
      "source": [
        "#### CREATE THE VGG16 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkDu2lrKAvqo"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.layers.convolutional import Conv2D\r\n",
        "from keras.layers.convolutional import MaxPooling2D\r\n",
        "from keras.layers.core import Activation\r\n",
        "from keras.layers.core import Flatten\r\n",
        "from keras.layers.core import Dropout\r\n",
        "from keras.layers.core import Dense\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "def VGG16(width, height, depth, classes):\r\n",
        "\t\t# initialize the model along with the input shape to be\r\n",
        "\t\t# \"channels last\" and the channels dimension itself\r\n",
        "\t\tmodel = Sequential()\r\n",
        "\t\tinputShape = (height, width, depth)\r\n",
        "\t\tchanDim = -1\r\n",
        "\t\t# CONV => RELU => POOL layer set\r\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\r\n",
        "\t\tmodel.add(Activation(\"relu\"))\r\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\r\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\t\t#model.add(Dropout(0.25))\r\n",
        "\r\n",
        "\t\t# (CONV => RELU) * 2 => POOL layer set\r\n",
        "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\r\n",
        "\t\tmodel.add(Activation(\"relu\"))\r\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\r\n",
        "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\r\n",
        "\t\tmodel.add(Activation(\"relu\"))\r\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\r\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\t\tmodel.add(Dropout(0.40))\r\n",
        "\r\n",
        "\t\t# (CONV => RELU) * 3 => POOL layer set\r\n",
        "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\r\n",
        "\t\tmodel.add(Activation(\"relu\"))\r\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\r\n",
        "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\r\n",
        "\t\tmodel.add(Activation(\"relu\"))\r\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\r\n",
        "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\r\n",
        "\t\tmodel.add(Activation(\"relu\"))\r\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\r\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\t\tmodel.add(Dropout(0.40))\r\n",
        "\r\n",
        "\t\t# first (and only) set of FC => RELU layers\r\n",
        "\t\tmodel.add(Flatten())\r\n",
        "\t\tmodel.add(Dense(512))\r\n",
        "\t\tmodel.add(Activation(\"relu\"))\r\n",
        "\t\tmodel.add(BatchNormalization())\r\n",
        "\t\tmodel.add(Dropout(0.5))\r\n",
        "\r\n",
        "\t\t# softmax classifier\r\n",
        "\t\tmodel.add(Dense(classes))\r\n",
        "\t\tmodel.add(Activation(\"softmax\"))\r\n",
        "\r\n",
        "\t\t# return the constructed network architecture\r\n",
        "\t\treturn model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NBU-pb5BUW0"
      },
      "source": [
        "model = VGG16(width=50, height=50, depth=1, classes=33)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HINrWgfYBqWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1257751f-de8d-4795-954e-83ee36ff8185"
      },
      "source": [
        "# and batch size\r\n",
        "INIT_LR = 0.01\r\n",
        "EPOCHS = 50\r\n",
        "BS = 16\r\n",
        "print(\"Model info... \\n \")\r\n",
        "print(model.summary())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model info... \n",
            " \n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 50, 50, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 50, 50, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 50, 50, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 25, 25, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 25, 25, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 25, 25, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 25, 25, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 25, 25, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 25, 25, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 25, 25, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 12, 12, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               2359808   \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 33)                16929     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 33)                0         \n",
            "=================================================================\n",
            "Total params: 2,805,729\n",
            "Trainable params: 2,803,617\n",
            "Non-trainable params: 2,112\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAzB2YU0dSTm",
        "outputId": "7708d8b6-b6ea-407e-f76c-e06749fef8ac"
      },
      "source": [
        "print(\"[INFO] training network...\")\r\n",
        "opt = keras.optimizers.Adam(lr= INIT_LR, decay= INIT_LR / EPOCHS)\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\r\n",
        "\tmetrics=[\"accuracy\"])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training network...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwojHKIGeLfw"
      },
      "source": [
        "# construct the image generator for data augmentation\r\n",
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\r\n",
        "\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\r\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxZeM1QgleAi"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV0b9Cz3ddgU",
        "outputId": "f034a706-04a1-40d5-e3fa-5ee887b4daf0"
      },
      "source": [
        "# train the network + monitor\r\n",
        "H = model.fit(x=x_train, y=keras.utils.to_categorical(y_train, 33), batch_size=BS,\r\n",
        "              validation_data=(x_test, keras.utils.to_categorical(y_test, 33)), \r\n",
        "              steps_per_epoch= len(y_train) // BS, epochs=EPOCHS)\r\n",
        "print(\"[INFO] training Done...\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
            "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1443/1443 [==============================] - 55s 34ms/step - loss: 1.1120 - accuracy: 0.7289 - val_loss: 0.3330 - val_accuracy: 0.9034\n",
            "Epoch 2/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.1978 - accuracy: 0.9437 - val_loss: 0.0971 - val_accuracy: 0.9803\n",
            "Epoch 3/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.1078 - accuracy: 0.9688 - val_loss: 0.0591 - val_accuracy: 0.9845\n",
            "Epoch 4/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.1053 - accuracy: 0.9710 - val_loss: 0.2705 - val_accuracy: 0.9148\n",
            "Epoch 5/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0797 - accuracy: 0.9774 - val_loss: 0.0435 - val_accuracy: 0.9913\n",
            "Epoch 6/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0683 - accuracy: 0.9813 - val_loss: 0.0418 - val_accuracy: 0.9932\n",
            "Epoch 7/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0597 - accuracy: 0.9837 - val_loss: 0.0627 - val_accuracy: 0.9871\n",
            "Epoch 8/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0400 - accuracy: 0.9883 - val_loss: 0.1057 - val_accuracy: 0.9894\n",
            "Epoch 9/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0516 - accuracy: 0.9857 - val_loss: 0.0454 - val_accuracy: 0.9928\n",
            "Epoch 10/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0480 - accuracy: 0.9882 - val_loss: 0.0485 - val_accuracy: 0.9932\n",
            "Epoch 11/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0306 - accuracy: 0.9920 - val_loss: 0.0413 - val_accuracy: 0.9936\n",
            "Epoch 12/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0311 - accuracy: 0.9912 - val_loss: 0.0364 - val_accuracy: 0.9955\n",
            "Epoch 13/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0208 - accuracy: 0.9938 - val_loss: 0.0380 - val_accuracy: 0.9951\n",
            "Epoch 14/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 0.0354 - val_accuracy: 0.9966\n",
            "Epoch 15/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0198 - accuracy: 0.9947 - val_loss: 0.0363 - val_accuracy: 0.9943\n",
            "Epoch 16/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0164 - accuracy: 0.9959 - val_loss: 0.0467 - val_accuracy: 0.9943\n",
            "Epoch 17/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0154 - accuracy: 0.9955 - val_loss: 0.0439 - val_accuracy: 0.9958\n",
            "Epoch 18/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 0.0586 - val_accuracy: 0.9951\n",
            "Epoch 19/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0097 - accuracy: 0.9974 - val_loss: 0.0398 - val_accuracy: 0.9958\n",
            "Epoch 20/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0159 - accuracy: 0.9959 - val_loss: 0.0344 - val_accuracy: 0.9966\n",
            "Epoch 21/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0097 - accuracy: 0.9973 - val_loss: 0.0489 - val_accuracy: 0.9958\n",
            "Epoch 22/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 0.0502 - val_accuracy: 0.9955\n",
            "Epoch 23/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.0463 - val_accuracy: 0.9962\n",
            "Epoch 24/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0089 - accuracy: 0.9977 - val_loss: 0.0479 - val_accuracy: 0.9966\n",
            "Epoch 25/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0070 - accuracy: 0.9983 - val_loss: 0.0439 - val_accuracy: 0.9966\n",
            "Epoch 26/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0094 - accuracy: 0.9975 - val_loss: 0.0504 - val_accuracy: 0.9958\n",
            "Epoch 27/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.0471 - val_accuracy: 0.9966\n",
            "Epoch 28/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0494 - val_accuracy: 0.9955\n",
            "Epoch 29/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.0402 - val_accuracy: 0.9958\n",
            "Epoch 30/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.0350 - val_accuracy: 0.9970\n",
            "Epoch 31/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.0444 - val_accuracy: 0.9962\n",
            "Epoch 32/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0465 - val_accuracy: 0.9970\n",
            "Epoch 33/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0435 - val_accuracy: 0.9962\n",
            "Epoch 34/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0469 - val_accuracy: 0.9955\n",
            "Epoch 35/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0528 - val_accuracy: 0.9970\n",
            "Epoch 36/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.0517 - val_accuracy: 0.9970\n",
            "Epoch 37/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0478 - val_accuracy: 0.9966\n",
            "Epoch 38/50\n",
            "1443/1443 [==============================] - 49s 34ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0482 - val_accuracy: 0.9962\n",
            "Epoch 39/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0497 - val_accuracy: 0.9962\n",
            "Epoch 40/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0492 - val_accuracy: 0.9966\n",
            "Epoch 41/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0498 - val_accuracy: 0.9955\n",
            "Epoch 42/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0490 - val_accuracy: 0.9970\n",
            "Epoch 43/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.0496 - val_accuracy: 0.9970\n",
            "Epoch 44/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0512 - val_accuracy: 0.9962\n",
            "Epoch 45/50\n",
            "1443/1443 [==============================] - 48s 34ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.0537 - val_accuracy: 0.9970\n",
            "Epoch 46/50\n",
            "1443/1443 [==============================] - 48s 33ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0474 - val_accuracy: 0.9970\n",
            "Epoch 47/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0481 - val_accuracy: 0.9966\n",
            "Epoch 48/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 0.0514 - val_accuracy: 0.9962\n",
            "Epoch 49/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0528 - val_accuracy: 0.9966\n",
            "Epoch 50/50\n",
            "1443/1443 [==============================] - 47s 33ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.0540 - val_accuracy: 0.9966\n",
            "[INFO] training Done...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}